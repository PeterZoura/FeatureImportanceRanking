{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9978181818181818\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "data_raw = pd.read_csv(\"../Data/data2.tsv\", sep=\"\\t\", index_col=0)\n",
    "data_XY = data_raw.drop(columns=data_raw.columns[0:1])\n",
    "X = data_XY.drop(columns=data_XY.columns[0:1])\n",
    "Y = data_XY.drop(columns=data_XY.columns[1:])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "# print(X_train)\n",
    "# print(Y_train)\n",
    "\n",
    "\n",
    "# nn = MLPClassifier(max_iter=1000, random_state=1, early_stopping=True)\n",
    "nn = MLPClassifier(max_iter=1000, random_state=1, early_stopping=True, hidden_layer_sizes=(\n",
    "    100, 50, 30), activation='tanh', solver='adam', alpha=0.001, learning_rate='constant')\n",
    "nn.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "y_pred = nn.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')  # Accuracy: 0.9986666666666667\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 20 100 50 30 3\n",
      "Ft Length has ave wt -0.003428709579537344\n",
      "Ft #Uppers has ave wt 0.0007199284017002877\n",
      "Ft #Lowers has ave wt 0.026919058776304485\n",
      "Ft #Numbers has ave wt 0.005339634613684806\n",
      "Ft #Symbols has ave wt 0.005114676294087909\n",
      "Ft SLG_uppers has ave wt -0.005337747580691614\n",
      "Ft SLG_lowers has ave wt 0.00019565183754592395\n",
      "Ft SLG_numbers has ave wt 0.012031678353494784\n",
      "Ft SLG_symbols has ave wt -0.0013776596325555778\n",
      "Ft FCT_Up has ave wt -0.011316849189902193\n",
      "Ft FCT_Lo has ave wt 0.0015882920876239485\n",
      "Ft FCT_Nu has ave wt -0.004260319534847005\n",
      "Ft FCT_Sy has ave wt -0.0035998117546341447\n",
      "Ft LCT_Up has ave wt 0.016525415344952245\n",
      "Ft LCT_Lo has ave wt -0.008302957542296803\n",
      "Ft LCT_Nu has ave wt -0.005247414705847608\n",
      "Ft LCT_Sy has ave wt 0.015882309851143896\n",
      "Ft #words has ave wt 0.018230552061529966\n",
      "Ft ave_size_of_words has ave wt 0.012963004073114863\n",
      "Ft palindrome has ave wt 0.005459434334350921\n",
      "And again\n",
      "\n",
      "#Lowershas avg wt 0.026919058776304485\n",
      "#wordshas avg wt 0.018230552061529966\n",
      "LCT_Uphas avg wt 0.016525415344952245\n",
      "LCT_Syhas avg wt 0.015882309851143896\n",
      "ave_size_of_wordshas avg wt 0.012963004073114863\n",
      "SLG_numbershas avg wt 0.012031678353494784\n",
      "palindromehas avg wt 0.005459434334350921\n",
      "#Numbershas avg wt 0.005339634613684806\n",
      "#Symbolshas avg wt 0.005114676294087909\n",
      "FCT_Lohas avg wt 0.0015882920876239485\n",
      "#Uppershas avg wt 0.0007199284017002877\n",
      "SLG_lowershas avg wt 0.00019565183754592395\n",
      "SLG_symbolshas avg wt -0.0013776596325555778\n",
      "Lengthhas avg wt -0.003428709579537344\n",
      "FCT_Syhas avg wt -0.0035998117546341447\n",
      "FCT_Nuhas avg wt -0.004260319534847005\n",
      "LCT_Nuhas avg wt -0.005247414705847608\n",
      "SLG_uppershas avg wt -0.005337747580691614\n",
      "LCT_Lohas avg wt -0.008302957542296803\n",
      "FCT_Uphas avg wt -0.011316849189902193\n"
     ]
    }
   ],
   "source": [
    "x = len(nn.coefs_)\n",
    "y = len(nn.coefs_[0])\n",
    "z1 = len(nn.coefs_[0][0])\n",
    "z2 = len(nn.coefs_[1][0])\n",
    "z3 = len(nn.coefs_[2][0])\n",
    "z4 = len(nn.coefs_[3][0])\n",
    "print(str(x) + \" \" + str(y) + \" \" + str(z1) + \" \" + str(z2) + \" \" + str(z3) + \" \" + str(z4))\n",
    "layer = 0\n",
    "neuron = 0\n",
    "\n",
    "featureAveWeights = {'Length': 0, '#Uppers': 0, '#Lowers': 0, '#Numbers': 0, '#Symbols': 0, 'SLG_uppers': 0, 'SLG_lowers': 0, 'SLG_numbers': 0, 'SLG_symbols': 0, 'FCT_Up': 0, 'FCT_Lo': 0, 'FCT_Nu': 0, 'FCT_Sy': 0, 'LCT_Up': 0, 'LCT_Lo': 0, 'LCT_Nu': 0, 'LCT_Sy': 0, '#words': 0, 'ave_size_of_words': 0, 'palindrome':0}\n",
    "\n",
    "for neuron in range(0, 100, 1):    \n",
    "    for feature in range(0, 20, 1):\n",
    "        featureAveWeights[X.columns[feature]] += nn.coefs_[layer][feature][neuron]\n",
    "\n",
    "\n",
    "for feature in range(0, 20, 1):\n",
    "    featureAveWeights[X.columns[feature]]/=100\n",
    "    print(\"Ft \" + X.columns[feature] + \" has ave wt \" + str(featureAveWeights[X.columns[feature]]))\n",
    "print(\"And again\\n\")\n",
    "\n",
    "avgRank = {k: v for k, v in sorted(featureAveWeights.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "x = 0\n",
    "for i in avgRank:\n",
    "    print(i + \"has avg wt \" + str(avgRank[i]))\n",
    "    x+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7881212121212121\n"
     ]
    }
   ],
   "source": [
    "X2 = data_XY.drop(columns=data_XY.columns[0:1])\n",
    "#'Length','#Uppers','#Lowers','#Numbers','#Symbols','SLG_uppers','SLG_lowers','SLG_numbers','SLG_symbols','FCT_Up','FCT_Lo','FCT_Nu','FCT_Sy','LCT_Up','LCT_Lo','LCT_Nu','LCT_Sy','#words','ave_size_of_words','palindrome'\n",
    "X2 = X2.drop(columns=['Length','#Uppers','#Numbers','#Symbols','SLG_uppers','SLG_lowers','SLG_numbers','SLG_symbols','FCT_Up','FCT_Lo','FCT_Nu','FCT_Sy','LCT_Lo','LCT_Nu','LCT_Sy','ave_size_of_words','palindrome'])\n",
    "X2_train, X2_test, Y_train, Y_test = train_test_split(\n",
    "    X2, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "# print(X_train)\n",
    "# print(Y_train)\n",
    "\n",
    "\n",
    "# nn = MLPClassifier(max_iter=1000, random_state=1, early_stopping=True)\n",
    "nn = MLPClassifier(max_iter=1000, random_state=1, early_stopping=True, hidden_layer_sizes=(\n",
    "    100, 50, 30), activation='tanh', solver='adam', alpha=0.001, learning_rate='constant')\n",
    "nn.fit(X2_train, Y_train.values.ravel())\n",
    "\n",
    "y_pred = nn.predict(X2_test)\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')  # Accuracy: 0.9986666666666667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
